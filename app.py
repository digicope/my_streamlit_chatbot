"""Streamlit ì›¹ ì±—ë´‡ ë©”ì¸ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸"""
import os
import streamlit as st
# from dotenv import load_dotenv

from src.llm import LLMClient
from src.prompts import DEFAULT_SYSTEM_PROMPT, DEFAULT_MODEL, DEFAULT_TEMPERATURE
from src.ui import render_sidebar, render_chat_messages, render_streaming_response
from src.utils import format_error_message, validate_api_key, logger

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Streamlit Web Chatbot",
    page_icon="ğŸ’¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

if "model" not in st.session_state:
    st.session_state.model = os.getenv("OPENAI_MODEL", DEFAULT_MODEL)

if "temperature" not in st.session_state:
    st.session_state.temperature = DEFAULT_TEMPERATURE

if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = DEFAULT_SYSTEM_PROMPT

# API í‚¤ ê²€ì¦
api_key = os.getenv("OPENAI_API_KEY")
is_valid, error_msg = validate_api_key(api_key)

if not is_valid:
    st.error(f"âš ï¸ {error_msg}")
    st.info("ğŸ’¡ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¡œ ì§€ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

# LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    llm_client = LLMClient(api_key=api_key)
except ValueError as e:
    st.error(f"âš ï¸ {str(e)}")
    st.stop()

# ë©”ì¸ UI
st.title("ğŸ’¬ Streamlit Web Chatbot")
st.caption("OpenAI APIë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€í™”í˜• ì±—ë´‡")

# ì‚¬ì´ë“œë°” ë Œë”ë§
model, temperature, system_prompt, should_reset = render_sidebar(
    default_model=st.session_state.model,
    default_temperature=st.session_state.temperature,
    default_system_prompt=st.session_state.system_prompt
)

# ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
st.session_state.model = model
st.session_state.temperature = temperature
st.session_state.system_prompt = system_prompt

# ì±„íŒ… ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ë Œë”ë§
render_chat_messages(st.session_state.messages)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_input := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.write(user_input)
    
    # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        # ë¡œë”© ì¸ë””ì¼€ì´í„°
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            try:
                # OpenAI API í¬ë§·ìœ¼ë¡œ ë©”ì‹œì§€ ë³€í™˜
                api_messages = []
                
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
                if st.session_state.system_prompt:
                    api_messages.append({
                        "role": "system",
                        "content": st.session_state.system_prompt
                    })
                
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì œì™¸)
                for msg in st.session_state.messages:
                    api_messages.append({
                        "role": msg["role"],
                        "content": msg["content"]
                    })
                
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
                response_placeholder = st.empty()
                stream_generator = llm_client.stream_chat(
                    messages=api_messages,
                    model=st.session_state.model,
                    temperature=st.session_state.temperature
                )
                
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë Œë”ë§
                full_response = render_streaming_response(
                    response_placeholder,
                    stream_generator
                )
                
                # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_response
                })
                
            except ValueError as e:
                error_message = str(e)
                st.error(f"âŒ {error_message}")
                logger.error(f"Error in chat: {error_message}")
            except Exception as e:
                error_message = format_error_message(e)
                st.error(f"âŒ {error_message}")
                logger.error(f"Unexpected error: {e}", exc_info=True)

# ì‚¬ì´ë“œë°” í•˜ë‹¨ì— ì •ë³´ í‘œì‹œ
with st.sidebar:
    st.divider()
    st.caption(f"ëª¨ë¸: {model}")
    st.caption(f"Temperature: {temperature}")
    st.caption(f"ëŒ€í™” ìˆ˜: {len(st.session_state.messages)}")
